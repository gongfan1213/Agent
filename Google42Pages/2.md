and outputs.

Functions work very similarly in the world of agents, but we can replace the software developer with a model. A model can take a set of known functions and decide when to use each Function and what arguments the Function needs based on its specification. Functions differ from Extensions in a few ways, most notably:
1. A model outputs a Function and its arguments, but doesn’t make a live API call.
2. Functions are executed on the client - side, while Extensions are executed on the agent - side.

Using our Google Flights example again, a simple setup for functions might look like the example in Figure 7.

Note that the main difference here is that neither the Function nor the agent interact directly with the Google Flights API. So how does the API call actually happen?

With functions, the logic and execution of calling the actual API endpoint is offloaded away from the agent and back to the client - side application as seen in Figure 8 and Figure 9 below. This offers the developer more granular control over the flow of data in the application. There are many reasons why a Developer might choose to use functions over Extensions, but a few common use cases are:
- API calls need to be made at another layer of the application stack, outside of the direct agent architecture flow (e.g. a middleware system, a front end framework, etc.)
- Security or Authentication restrictions that prevent the agent from calling an API directly (e.g API is not exposed to the internet, or non - accessible by agent infrastructure)
- Timing or order - of - operations constraints that prevent the agent from making API calls in real - time. (i.e. batch operations, human - in - the - loop review, etc.)
- Additional data transformation logic needs to be applied to the API Response that the agent cannot perform. For example, consider an API endpoint that doesn’t provide a filtering mechanism for limiting the number of results returned. Using Functions on the client - side provides the developer additional opportunities to make these transformations.
- The developer wants to iterate on agent development without deploying additional infrastructure for the API endpoints (i.e. Function Calling can act like “stubbing” of APIs)

While the difference in internal architecture between the two approaches is subtle as seen in Figure 8, the additional control and decoupled dependency on external infrastructure makes Function Calling an appealing option for the Developer.

### Use cases
A model can be used to invoke functions in order to handle complex, client - side execution flows for the end user, where the agent Developer might not want the language model to manage the API execution (as is the case with Extensions). Let’s consider the following example where an agent is being trained as a travel concierge to interact with users that want to book vacation trips. The goal is to get the agent to produce a list of cities that we can use in our middleware application to download images, data, etc. for the user’s trip planning. A user might say something like:

I’d like to take a ski trip with my family but I’m not sure where to go.

In a typical prompt to the model, the output might look like the following:

Sure, here’s a list of cities that you can consider for family ski trips:
- Crested Butte, Colorado, USA
- Whistler, BC, Canada
- Zermatt, Switzerland

While the above output contains the data that we need (city names), the format isn’t ideal for parsing. With Function Calling, we can teach a model to format this output in a structured style (like JSON) that’s more convenient for another system to parse. Given the same input prompt from the user, an example JSON output from a Function might look like Snippet 5 instead.

```json
function_call { 
    name: "display_cities",
    args: { 
        "cities": ["Crested Butte", "Whistler", "Zermatt"], 
        "preferences": "skiing" 
    } 
}
```
This JSON payload is generated by the model, and then sent to our Client - side server to do whatever we would like to do with it. In this specific case, we’ll call the Google Places API to take the cities provided by the model and look up Images, then provide them as formatted rich content back to our User. Consider this sequence diagram in Figure 9 showing the above interaction in step - by - step detail.

The result of the example in Figure 9 is that the model is leveraged to “fill in the blanks” with the parameters required for the Client side UI to make the call to the Google Places API. The Client side UI manages the actual API call using the parameters provided by the model in the returned Function. This is just one use case for Function Calling, but there are many other scenarios to consider like:
- You want a language model to suggest a function that you can use in your code, but you don't want to include credentials in your code. Because function calling doesn't run the function, you don't need to include credentials in your code with the function information.
- You are running asynchronous operations that can take more than a few seconds. These scenarios work well with function calling because it's an asynchronous operation.
- You want to run functions on a device that's different from the system producing the function calls and their arguments.

One key thing to remember about functions is that they are meant to offer the developer much more control over not only the execution of API calls, but also the entire flow of data in the application as a whole. In the example in Figure 9, the developer chose to not return API information back to the agent as it was not pertinent for future actions the agent might take. However, based on the architecture of the application, it may make sense to return the external API call data to the agent in order to influence future reasoning, logic, and action choices. Ultimately, it is up to the application developer to choose what is right for the specific application.

### Function sample code
To achieve the above output from our ski vacation scenario, let’s build out each of the components to make this work with our gemini - 1.5 - flash - 001 model.

First, we’ll define our display_cities function as a simple Python method.
```python
from typing import Optional

def display_cities(cities: list[str], preferences: Optional[str] = None):
    """Provides a list of cities based on the user's search query and preferences.
    Args:
        preferences (str): The user's preferences for the search, like skiing,
        beach, restaurants, bbq, etc.
        cities (list[str]): The list of cities being recommended to the user.
    Returns:
        list[str]: The list of cities being recommended to the user. 
    """
    return cities
```
Next, we’ll instantiate our model, build the Tool, then pass in our user’s query and tools to the model. Executing the code below would result in the output as seen at the bottom of the code snippet.
```python
from vertexai.generative_models import GenerativeModel, Tool, FunctionDeclaration

model = GenerativeModel("gemini - 1.5 - flash - 001")
display_cities_function = FunctionDeclaration.from_func(display_cities)
tool = Tool(function_declarations=[display_cities_function])
message = "I’d like to take a ski trip with my family but I’m not sure where to go."
res = model.generate_content(message, tools=[tool])
print(f"Function Name: {res.candidates[0].content.parts[0].function_call.name}")
print(f"Function Args: {res.candidates[0].content.parts[0].function_call.args}")
```
```
> Function Name: display_cities
> Function Args: {'preferences':'skiing', 'cities': ['Aspen', 'Vail', 'Park City']}
```
In summary, functions offer a straightforward framework that empowers application developers with fine - grained control over data flow and system execution, while effectively leveraging the agent/model for critical input generation. Developers can selectively choose whether to keep the agent “in the loop” by returning external data, or omit it based on specific application architecture requirements.

### Data stores
Imagine a language model as a vast library of books, containing its training data. But unlike a library that continuously acquires new volumes, this one remains static, holding only the knowledge it was initially trained on. This presents a challenge, as real - world knowledge is constantly evolving. Data Stores address this limitation by providing access to more dynamic and up - to - date information, and ensuring a model’s responses remain grounded in factuality and relevance.

Consider a common scenario where a developer might need to provide a small amount of additional data to a model, perhaps in the form of spreadsheets or PDFs.

Data Stores allow developers to provide additional data in its original format to an agent, eliminating the need for time - consuming data transformations, model retraining, or fine - tuning. The Data Store converts the incoming document into a set of vector database embeddings that the agent can use to extract the information it needs to supplement its next action or response to the user.

In the context of Generative AI agents, Data Stores are typically implemented as a vector database that the developer wants the agent to have access to at runtime. While we won’t cover vector databases in depth here, the key point to understand is that they store data in the form of vector embeddings, a type of high - dimensional vector or mathematical representation of the data provided. One of the most prolific examples of Data Store usage with language models in recent times has been the implementation of Retrieval Augmented Generation (RAG) based applications. These applications seek to extend the breadth and depth of a model’s knowledge beyond the foundational training data by giving the model access to data in various formats like:
- Website content
- Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.
- Unstructured Data in formats like HTML, PDF, TXT, etc.

The underlying process for each user request and agent response loop is generally modeled as seen in Figure 13.
1. A user query is sent to an embedding model to generate embeddings for the query
2. The query embeddings are then matched against the contents of the vector database using a matching algorithm like SCaNN
3. The matched content is retrieved from the vector database in text format and sent back to the agent
4. The agent receives both the user query and retrieved content, then formulates a response or action
5. A final response is sent to the user

The end result is an application that allows the agent to match a user’s query to a known data store through vector search, retrieve the original content, and provide it to the orchestration layer and model for further processing. The next action might be to provide a final answer to the user, or perform an additional vector search to further refine the results.

A sample interaction with an agent that implements RAG with ReAct reasoning/planning can be seen in Figure 14.

## Implementation and application
In the context of Generative AI agents, the components we've discussed - models, tools, and the orchestration layer - come together in various ways. For example, in a customer service application, an agent might use a model to understand a customer's query. Based on the query, it could select an appropriate tool, like an Extension to access a customer database or a Function to format the response in a specific way. The orchestration layer would manage the flow, ensuring that the agent takes the right actions at each step, using techniques like ReAct or Chain - of - Thought to reason about the best course of action.

### Tools recap
To summarize, extensions, functions and data stores make up a few different tool types available for agents to use at runtime. Each has their own purpose and they can be used together or independently at the discretion of the agent developer.
| | Extensions | Function Calling | Data Stores |
|--|--|--|--|
| Execution | Agent - Side Execution | Client - Side Execution | Agent - Side Execution |
| Use Case | Developer wants agent to control interactions with the API endpoints; Useful when leveraging native pre - built Extensions (i.e. Vertex Search, Code Interpreter, etc.) | API calls need to be made at another layer of the application stack; Security or Authentication restrictions prevent the agent from calling an API directly; Timing constraints or order - of - operations constraints that prevent the agent from making API calls in real - time; Additional data transformation logic needs to be applied to the API Response that the agent cannot perform; The developer wants to iterate on agent development without deploying additional infrastructure for the API endpoints | Developer wants to implement Retrieval Augmented Generation (RAG) with any of the following data types: Website Content from pre - indexed domains and URLs; Structured Data in formats like PDF, Word Docs, CSV, Spreadsheets, etc.; Unstructured Data in formats like HTML, PDF, TXT, etc.; Relational / Non - Relational Databases |

### Enhancing model performance with targeted learning
A crucial aspect of using models effectively is their ability to choose the right tools when generating output, especially when using tools at scale in production. While general training helps models develop this skill, real - world scenarios often require knowledge beyond the training data. Imagine this as the difference between basic cooking skills and mastering a specific cuisine. Both require foundational cooking knowledge, but the latter demands targeted learning for more nuanced results.

To help the model gain access to this type of specific knowledge, several approaches exist:
- In - context learning: This method provides a generalized model with a prompt, tools, and few - shot examples at inference time which allows it to learn ‘on the fly' how and when to use those tools for a specific task. The ReAct framework is an example of this approach in natural language.
- Retrieval - based in - context learning: This technique dynamically populates the model prompt with the most relevant information, tools, and associated examples by retrieving them from external memory. An example of this would be the ‘Example Store’ in Vertex AI extensions or the data stores RAG based architecture mentioned previously.
- Fine - tuning based learning: This method involves training a model using a larger dataset of specific examples prior to inference. This helps the model understand when and how to apply certain tools prior to receiving any user queries.

To provide additional insights on each of the targeted learning approaches, let’s revisit our cooking analogy.
- Imagine a chef has received a specific recipe (the prompt), a few key ingredients (relevant tools) and some example dishes (few - shot examples) from a customer. Based on this limited information and the chef’s general knowledge of cooking, they will need to figure out how to prepare the dish ‘on the fly’ that most closely aligns with the recipe and the customer’s preferences. This is in - context learning.
- Now let’s imagine our chef in a kitchen that has a well - stocked pantry (external data stores) filled with various ingredients and cookbooks (examples and tools). The chef is now able to dynamically choose ingredients and cookbooks from the pantry and better align to the customer’s recipe and preferences. This allows the chef to create a more informed and refined dish leveraging both existing and new knowledge. This is retrieval - based in - context learning.
- Finally, let’s imagine that we sent our chef back to school to learn a new cuisine or set of cuisines (pre - training on a larger dataset of specific examples). This allows the chef to approach future unseen customer recipes with deeper understanding. This approach is perfect if we want the chef to excel in specific cuisines (knowledge domains). This is fine - tuning based learning.

Each of these approaches offers unique advantages and disadvantages in terms of speed, cost, and latency. However, by combining these techniques in an agent framework, we can leverage the various strengths and minimize their weaknesses, allowing for a more robust and adaptable solution.

### Agent quick start with LangChain
In order to provide a real - world executable example of an agent in action, we’ll build a quick prototype with the LangChain and LangGraph libraries. These popular open - source libraries allow users to build customer agents by “chaining” together sequences of logic, reasoning, and tool calls to answer a user’s query. We’ll use our gemini - 1.5 - flash - 001 model and some simple tools to answer a multi - stage query from the user as seen in Snippet 8.

The tools we are using are the SerpAPI (for Google Search) and the Google Places API. After executing our program in Snippet 8, you can see the sample output in Snippet 9.
```python
import os
from langgraph.prebuilt import create_react_agent
from langchain_core.tools import tool
from langchain_community.utilities import SerpAPIWrapper
from langchain_community.tools import GooglePlacesTool

os.environ["SERPAPI_API_KEY"] = "XXXXX"
os.environ["GPLACES_API_KEY"] = "XXXXX"

@tool
def search(query: str): 
    """Use the SerpAPI to run a Google Search."""
    search = SerpAPIWrapper()
    return search.run(query)

@tool
def places(query: str):
    """Use the Google Places API to run a Google Places Query."""
    places = GooglePlacesTool()
    return places.run(query)

from vertexai.preview.language_models import ChatVertexAI
model = ChatVertexAI(model="gemini - 1.5 - flash - 001")
tools = [search, places]
query = "Who did the Texas Longhorns play in football last week? What is the address of the other team's stadium?"
agent = create_react_agent(model, tools) 
input = {"messages": [("human", query)]}
for s in agent.stream(input, stream_mode="values"):
    message = s["messages"][-1]
    if isinstance(message, tuple):
        print(message)
    else:
        message.pretty_print()
```
```
=============================== Human Message ================================
Who did the Texas Longhorns play in football last week? What is the address of the other team's stadium?
================================= Ai Message =================================
Tool Calls: search
Args:
query: Texas Longhorns football schedule
================================ Tool Message ================================
Name: search
{...Results: "NCAA Division I Football, Georgia, Date..."}
================================= Ai Message =================================
The Texas Longhorns played the Georgia Bulldogs last week. Tool Calls: places
Args:
query: Georgia Bulldogs stadium
================================ Tool Message ================================
Name: places
..........:
{...Sanford Stadium Address: 100 Sanford...}
================================= Ai Message =================================
The address of the Georgia Bulldogs stadium is 100 Sanford Dr, Athens, GA 30602, USA.
```
While this is a fairly simple agent example, it demonstrates the foundational components of Model, Orchestration, and tools all working together to achieve a specific goal. In the final section, we’ll explore how these components come together in Google - scale managed products like Vertex AI
