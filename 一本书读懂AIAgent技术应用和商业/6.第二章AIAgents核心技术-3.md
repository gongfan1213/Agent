### 第2章 AI Agent核心技术
数据，并根据该数据采取行动以实现特定目标。AI Agent能够模拟智能行为，可以像基于规则的系统一样简单，也可以像高级机器学习模型一样复杂。它们一般使用预先确定的规则或经过训练的模型来做出决策，也可能需要外部控制或监督。

1.3.2节介绍过，根据设计目的和交互方式，AI Agent可以分为自主Agent和生成式Agent。而其中，自主Agent是一种高级软件程序，可以在没有人为控制的情况下独立运行。它可以自己思考、行动和学习，不需要人类持续输入。这些Agent可以广泛应用于医疗保健、金融和银行等不同行业，使事情运行得更顺畅、更高效。它们可以适应新情况，从经验中学习，并使用自身的内部系统做出决定。从目前已经公布的论文及研究来看，AI Agent的结构主要分为四模块结构和三模块结构，下面分别进行说明。

#### 1. 四模块结构

通用AI Agent的核心通常包含四个组件：环境、传感器、执行器和决策机制。

- **环境**：AI Agent在其中运行的区域或域，可以是物理空间，如工厂车间，也可以是数字空间，如网站。

- **传感器**：AI Agent用来感知其环境的工具，可以是摄像头、麦克风或任何其他感知输入工具，AI Agent可以使用它们来了解周围发生的事情。 

- **执行器**：AI Agent用来与其环境交互的工具，可以是机械臂、计算机屏幕或任何AI Agent可用于改变环境的其他设备。 

- **决策机制**：AI Agent的大脑，用于处理传感器收集的信息，并决定使用执行器采取什么行动。决策机制是体现AI Agent主动性与反应能力的重要部分，AI Agent使用各种决策机制，例如基于规则的系统、专家系统和神经网络等，以做出明智的选择并有效地执行任务。

图2-8为通用AI Agent框架结构，这种通用结构一定程度上启发了AI Agent的四模块结构。


![image](https://github.com/user-attachments/assets/6972b178-678c-4ea2-9ea1-c35be6f9e1b3)


学习系统可以使AI Agent从其经验和与环境的交互中学习，使用强化学习、监督学习和无监督学习等技术来持续提高AI Agent的性能。它就像一个监督检查，通过基于数据、事件等的学习来不断提升AI Agent的能力。需要说明的是，它属于事后的总结与反馈，并不参与Agent的具体执行环节。通过了解环境、传感器、执行器和决策机制，开发人员可以创建AI Agent以准确高效地执行特定任务。

# 案例：中国人民大学高瓴人工智能学院的四模块框架
对于Agent的构建，中国人民大学高瓴人工智能学院在论文“A Survey on Large Language Model based Autonomous Agents” 中也提出一种“四模块”Agent统一框架，四个模块分别是表示Agent属性的分析模块、存储历史信息的记忆模块、制定未来行动策略的规划模块和执行规划决定的行动模块。该框架如图2-9所示。


![image](https://github.com/user-attachments/assets/7049650c-49c5-46e0-9c11-24643a5ee4b9)


**图2-8 通用AI Agent框架结构**
（图中展示了学习、世界知识记忆、感知、推理、交互、行动之间的关系）

**图2-9 基于LLM的自主Agent体系结构设计统一框架**

 - **分析模块**：生成战略（手工制作方法、GPT生成方法、数据集对齐方法）、个人资料内容（人口统计信息、个人信息、社会信息），旨在识别Agent是什么角色。
 - **记忆模块**：记忆结构（长期记忆、短期记忆）、记忆格式（语言、数据库、列表）、记忆操作（记忆读、记忆写、记忆反思），存储历史信息。
 - **规划模块**：无反馈规划（子目标分解、多路径思考、外部规划器）、有反馈规划（环境反馈、人类反馈、模型反馈），制定未来行动策略。
 - **行动模块**：行动目标（对话、任务完成、探索）、行动策略（回忆、互动、使用工具、不使用工具）、环境（行动影响、内部状态、新行动、人类），执行规划决定。

四个模块的功能为：分析模块旨在识别Agent是什么角色；记忆模块和规划模块可将Agent置于动态环境中，使Agent能够回忆过去的行为并计划未来的行动；行动模块负责将Agent的决策转化为具体的输出。在这些模块中，分析模块影响记忆模块和规划模块，这三个模块共同影响行动模块。

**（1）分析模块**

自主Agent通过特定角色，例如程序员、教师和领域专家来执行任务。分析模块旨在表明Agent的角色是什么，这些信息通常被写入输入提示中以影响LLM的行为。在现有的工作中，有三种常用的策略来生成Agent配置文件：手工制作方法、GPT生成方法和数据集对齐方法。

**（2）记忆模块**

记忆模块在AI Agent的构建中起着非常重要的作用。它记忆从环境中感知到的信息，并利用记录的记忆来促进Agent未来的动作。记忆模块可以帮助Agent积累经验、实现自我进化，并以更加一致、合理、有效的方式完成任务。

**（3）规划模块**

人类在面临复杂任务时，首先会将其分解为多个简单的子任务，然后逐一解决。规划模块赋予基于LLM的AI Agent解决复杂任务时需要的思考和规划能力，使Agent更加全面、强大、可靠。这篇论文介绍了两种规划模块：无反馈规划与有反馈规划。

**（4）行动模块**

行动模块旨在将Agent的决策转化为具体的结果输出。它直接与环境交互，决定Agent完成任务的有效性。

除了四个模块，这个框架还包含Agent学习策略，包括从示例中学习、从环境反馈中学习、从交互的人类反馈中学习。

#### 2. 三模块结构
三模块结构的AI Agent，如复旦大学NLP团队在“The Rise and Potential of Large Language Model Based Agents: A Survey”论文中提出的“大脑、感知、行动”框架，如图2-10所示。

在这个框架中，大脑主要由一个LLM组成，不仅存储知识和记忆，还承担着信息处理和决策等功能，并可以呈现推理和规划的过程，能很好地应对未知任务。在大脑模块的运行机制中，自然语言交互能力在交流中起到至关重要的作用。接收感知模块处理的信息后，大脑模块首先进行知识检索和回忆。这些结果有助于Agent制订计划、进行推理和做出明智的决定。大脑模块还能以摘要、向量或其他数据结构的形式记忆Agent过去的观察、思考和行动，并更新常识和领域知识等知识以备将来使用。此外，基于LLM的AI Agent还具备适应陌生场景的概括和迁移能力。


![image](https://github.com/user-attachments/assets/ff5e59e6-998f-4aae-8734-06dcec5c503d)


**图2-10 三模块结构**

 - **环境**：包含示例对话，如“看现在的天空，你认为明天会下雨吗？如果你认为会下雨，那就把伞给我吧。”“从现在的天气情况和网上的天气预报来推断，明天很有可能会下雨，给你伞。”
 - **感知**：输入（文本输入、视觉输入、听觉输入及其他输入如触觉、嗅觉、温度、湿度、亮度等）。
 - **大脑**：存储（记忆、知识），功能包括总结、回忆、学习、检索、决策制定、规划/推理、泛化/迁移 。
 - **行动**：文本输出、调用API、工具使用（理解工具、使用工具、制作工具等）、具身行动（可将模型智能与物理世界结合起来） 。

该架构的整体结构上，大脑模块主要包括自然语言交互（图中未列出）、知识（语言知识、常识知识、专业领域知识）、记忆、规划/推理、泛化/迁移等五大部分；感知模块包括文本输入、视觉输入、听觉输入及其他输入（触觉、嗅觉、温度、湿度、亮度等）四大部分；行动模块包括文本输出、工具使用（理解工具、使用工具、制作工具等）、具身行动（可将模型智能与物理世界结合起来）等几个部分。

### 2.3.2 AI Agent的主流架构
2023年6月底，来自OpenAI公司的人工智能工程师翁丽莲（英文名：lilianweng）发表了一篇名为“LLM Powered Autonomous Agents”的博文，对基于LLM的AI Agent做了系统综述。在这篇文章中，她将自主Agent定义为LLM、记忆（Memory）、规划（Planning Skill）以及工具使用（Tool Use）的集合，其中LLM是核心大脑，记忆、任务规划以及工具使用等则是Agent系统实现的三个关键模块（组件）。她还对每个模块的实现路径进行了细致的梳理和说明，并通过AutoGPT、GPT-Engineer等案例进行了实例验证。

这篇阐述LLM赋能的自主Agent文章很快就得到了业内认可，其中给出的自主Agent架构也成为AI Agent的主流架构。LLM驱动的自主Agent系统架构如图2-11所示。这里探索的是基于LLM的AI Agent，虽然图2-11中没有具体体现LLM模块，但毫无疑问，担当推理与规划的LLM是AI Agent的核心所在。


![image](https://github.com/user-attachments/assets/93aee103-5398-4c36-ba29-d78845ee9934)


**图2-11 LLM驱动的自主Agent系统架构**

 - **记忆**：包括短期记忆、长期记忆。
 
 - **工具使用**：可调用Calendar()、Calculator()、CodeInterpreter()、Search()等工具 。
 - **Agent**：与记忆、规划、工具使用、行动相关联 。
 - **规划**：包含反思、自我批评、思维链、子目标分解 。
 - **行动**：与工具使用以虚线相连 。

这个架构也属于四模块结构，但它能够更直观地体现Agent的运作机制，通过“类人”思考与工作的形式，让以AI Agent为代表的人工智能应用更加直观。对比人类与外部环境交互的过程，我们会发现人类基于对世界的全面感知，推导出隐藏的状态，并结合自己的记忆和对世界的知识理解来做出计划、决策和行动。这些行动会反作用于环境，为我们提供新的反馈，我们再结合对反馈的观察做出新的决策，如此循环往复。

这里需要对此架构中的“行动（Action）”这个选项做一下说明。在图2-11中，细心的读者可能会发现，Agent架构图中还有一个与“工具使用”模块以虚线相连接的“行动（Action）”选项。

关于此架构的另一种解读方法是单纯分析AI Agent架构，将行动模块作为AI Agent的一个模块，把Agent的四大模块分为规划、记忆、工具使用和行动，而行动部分是Agent实际执行决定或响应的部分。AI Agent基于规划和记忆来执行具体的行动，这些行动可能包括与外部世界的互动，也可能是通过工具的调用来完成一个动作（任务）。面对不同的任务，Agent系统有一个完整的行动策略集，在决策时可以选择需要执行的行动，比如我们熟知的记忆检索、推理、学习、编程等。

翁丽莲在其阐述AI Agent的博文中没有特别提到“行动”部分，本书认为“行动”部分可以包含在“工具使用”模块中，因为工具使用就是为了付诸行动以完成各种任务。当然，不管是哪一种解读方法，都能帮助大家更好地理解AI Agent。

所以模仿人类的行为，AI Agent最直接的公式为：Agent = LLM +规划+记忆+工具使用（Tool Use + Action）。在这个公式中，我们需要注意，在制订计划的过程中，除了要考虑当前的状态，还需要利用记忆、经验、对过往的反思和总结，以及世界知识。

这个四模块结构是较为理想的Agent架构，也是目前最为主流的Agent架构。OpenAI所定义的基于LLM的自主Agent体系里，LLM发挥大脑功能，其他模块（组件）作为能力补充，这些模块的主要功能如下。

**（1）规划**

 - **子目标分解**：AI Agent将复杂的大任务分解成多个较小、可管理的子目标，以便有效处理复杂任务。
 - **反思与优化**：AI Agent可以对过去的行动进行自我批评和反思，从错误中学习，并在未来的步骤中进行改进，以此不断提高最终结果的质量。

**（2）记忆**

 - **短期记忆**：AI Agent在执行任务时暂时存储和快速访问信息的能力。短期记忆一般源自LLM的上下文学习，比如提示工程（Prompt Engineering）就利用了短期记忆来学习。
 - **长期记忆**：AI Agent存储知识、经验或学习到的模式的能力，这些信息可以保留很长时间，甚至无限期。具备长期记忆的AI Agent一般会有外部向量存储，AI Agent可以在查询时进行访问，实现快速检索，以获取大量信息。

**（3）工具使用**

AI Agent可以调用外部API获得模型权重中所没有的额外信息，包括实时信息、代码执行能力、对专属信息源的访问等。

2.2.3节已经介绍了LLM为何能够胜任Agent的大脑，这里不再赘述。下面，我们展开介绍AI Agent的规划、记忆和工具使用三个模块。

### 2.3.3 AI Agent的主要模块
#### 1. 规划
一个复杂的任务通常包含多个步骤，AI Agent需要明确这些步骤并提前制订计划。

**（1）任务分解**

 - **思维链（Chain of Thought，CoT）**：已成为提升模型处理复杂任务能力的标准化提示技术。提示模型“一步步思考”能够利用更多的在线计算将难题分解成更小、更简单的步骤。CoT将大任务转化为多个可管理的子目标，同时洞察模型的思维过程。
 - **思维树（Tree of Thoughts，ToT）**：进一步扩展了CoT，在每个步骤中探索多种可能的推理路径。它首先将问题分解为多个思维步骤，并为每个步骤生成多个思路，形成一个树状结构。搜索过程可以采用广度优先搜索（BFS）或深度优先搜索（DFS），每个状态由一个提示符评估或通过多数投票评估。 
 - **LLM+P**：它依赖于一个外部经典规划器进行长期规划。该方法使用规划域定义语言（Planning Domain Definition Language，PDDL）作为中间接口，描述规划问题。在这个过程中，LLM首先将问题转换为PDDL，然后请求经典规划器根据现有的领域PDDL生成PDDL计划，最后将PDDL计划转换回自然语言。从本质上讲，规划步骤被外包给了一个外部工具，这需要特定领域的PDDL和合适的规划器。这种方法在某些机器人设置中很常见，但在其他领域并不常用。

**（2）自我反思**
自我反思是一种重要的机制，使自主Agent能够通过对过去行动和决策的优化与错误修正逐步进行改进。它在需要反复试验和纠正错误的实际任务中发挥至关重要的作用。ReAct（Reasoning and Acting）将推理和行动结合在LLM内部，通过将行动空间扩展为任务特定的离散动作和语言空间的组合来实现。前者使LLM能够与环境交互（例如使用Wikipedia搜索API），而后者提示LLM以自然语言生成推理轨迹。


![image](https://github.com/user-attachments/assets/0c24f69c-8ff1-407c-a8c2-6fc12c63cebd)


ReAct提示模板（ReAct prompt template）结合了明确的思考、行动和观察步骤，其结构如下：
 - **思考**：……
 - **行动**：……
 - **观察**：……
 - **重复多次**

在知识密集型任务（如HotpotQA、FEVER）和决策制定任务（如ALFWorld、WebShop）的试验中，与仅包含行动的基准相比，ReAct表现更佳。这表明自我反思对提高决策质量很重要。图2-12为知识密集型任务（以HotpotQA为例）和决策制定任务（以ALFWorld Env为例）的推理轨迹示例。


![image](https://github.com/user-attachments/assets/0d8a1aba-a270-4fed-918d-a263a8996f1f)


**图2-12 知识密集型任务和决策制定任务的推理轨迹示例**

 - **HotpotQA**：展示了思考、行动、观察的步骤示例 。
 - **ALFWorld Env**：展示了思考、行动、观察的步骤示例 。

Reflexion是一个框架，利用动态记忆和自我反思能力来提升Agent的推理技能。Reflexion遵循标准的强化学习设置，其中奖励模型提供简单的二进制奖励。动作空间遵循ReAct中的设置，将特定任务的动作空间与语言相结合，以实现复杂的推理步骤。Reflexion框架如图2-13所示。

**图2-13 Reflexion框架**
 - 包含查询、LLM、行动、环境、奖励、反思（LLM）、Heuristic（h）等元素 。
 - 有{a0, a0', a1, a1', a2, a2', …, aN, aN'}、{r0, r1, r2, …, rN}等符号表示 。


![image](https://github.com/user-attachments/assets/1b315a03-62d3-4349-85ff-c196b4a964ad)


在每个动作之后，Agent会计算一个启发式函数，并根据自我反思结果决定是否重置环境以开始新的试验。启发式函数用于判断轨迹效率是否低下或是否存在幻觉。效率低下的规划是指花费大量时间却未能成功的轨迹。幻觉被定义为出现连续相同的动作序列，导致环境中观察结果相同的情况。

自我反思是通过向LLM展示两份示例来创建的，每份示例都包含一对“失败的轨迹”和“用于指导未来计划变更的理想反思”。然后，反思被添加到Agent的工作记忆中，最多三个，作为查询LLM的上下文。在ALFWorld Env（结合了复杂的任务导向和语言理解的研究环境）和HotpotQA（面向自然语言和多步推理问题的新型问答数据集）的试验中，幻觉是更常见的失败形式。这表明，自我反思机制能够检测并纠正LLM的缺陷。

回顾链（Chain of Hindsight，CoH）算法鼓励模型使用自身输出历史序列增强自身，当然每个历史都是有标注的。CoH这个想法展现了一种基于历史聊天上下文去增强模型输出效果的能力。算法蒸馏（Algorithm Distillation）应用了相同的想法，在增强学习任务中使用跨片段数据，这个算法被封装在一个很长的基于对历史对话选择的策略中。考虑到Agent可能与外部环境发生多次交互且每次交互都有可能得到更好的输出，算法蒸馏把基于历史的学习整理起来再喂给模型，这样模型在未来会表现得更好。

#### 2. 记忆
**（1）记忆的种类**
记忆可以被定义为获取、存储、保留和


### 2. 记忆
**（1）记忆的种类**
记忆可以被定义为获取、存储、保留和事后取回信息的过程，主要存在于人类大脑中。记忆可分为以下几类。
 - **感知记忆**：这是记忆的早期形态，能够保留感知信息（如视觉和听觉信息），即便感官活动结束。感知记忆的持续时间通常为几秒。其子分类包括视觉、听觉、触觉等。 
 - **短期记忆或工作记忆**：这种记忆形式用于处理复杂的认知工作，如学习和推理等任务。短期记忆被认为是有限的，一般只能存储7项左右的信息，并且存储时间通常为20 - 30秒。 
 - **长期记忆**：长期记忆可以存储长时间的信息，从几天到几十年不等，且具有几乎无限的存储空间。长期记忆分为以下两类：
    - **明确/陈述性记忆**：这种记忆主要涉及事实和事件，这些信息可以被有意识地回忆和召回。明确/陈述性记忆包括跨时期的事实和情节，以及对事实的理解。
    - **隐式/程序性记忆**：这种类型的记忆是无意识的，通常包括技能、惯性动作和不由自主的条件反射等，比如骑自行车或基于键盘打字等活动就属于隐式/程序性记忆的范畴。

可以将不同类型的记忆进行如下映射：
 - **感知记忆**：可以映射为基于原始输入（比如文本、图片或其他模态）做embedding。 
 - **短期记忆**：可以看作上下文（in - context）中的学习，其持续时间较短且有限。例如，在Transformer架构中，上下文窗口（context window）的大小、LLM的输入提示令牌（input prompt token）的长度限制都属于短期记忆的范畴。 
 - **长期记忆**：通常是指外部的向量数据库（vector store），Agent可以在查询时快速访问并检索数据。

**（2）MIPS（Maximum Inner Product Search，最大内积搜索）算法**
外部记忆可以减轻有限注意力范围的约束。一种标准做法是将信息的嵌入（embedding）表示保存在向量数据库中，支持快速的最大内积搜索。为了提升检索速度，常用ANN（Approximate Nearest Neighbor，近似最近邻）算法返回最相近的前k个近邻，代价是损失一定的准确性。

下面是一些常见的ANN算法。
 - **LSH（Locality Sensitive Hashing，局部敏感哈希）**：一种在高维数据空间中寻找近似最近邻的高效算法。其核心思想是通过设计一种特殊的哈希函数，使得在原始空间中距离较近的点在哈希空间中也有较高的概率发生碰撞（即被映射到同一个桶中） ，从而能够快速找到近似最近邻。 
